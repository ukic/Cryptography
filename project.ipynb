{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.12/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.venv/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio pandas numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load fashion mnist dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "fmnist_train = datasets.FashionMNIST('fashionMNIST', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# CNN model\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = torch.nn.Linear(5*5*64, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 5*5*64)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# train model\n",
    "def train(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 1000 == 999:\n",
    "                print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 1000}')\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1000] loss: 0.7596919068086426\n",
      "[1, 2000] loss: 0.49098808356636436\n",
      "[1, 3000] loss: 0.4371311948732473\n",
      "[1, 4000] loss: 0.41725688544598233\n",
      "[1, 5000] loss: 0.39605649235812596\n",
      "[1, 6000] loss: 0.3875735186637321\n",
      "[1, 7000] loss: 0.3690632801573593\n",
      "[1, 8000] loss: 0.36051354347725645\n",
      "[1, 9000] loss: 0.3483217911435722\n",
      "[1, 10000] loss: 0.36706697714936615\n",
      "[1, 11000] loss: 0.328453355159408\n",
      "[1, 12000] loss: 0.3294036126303781\n",
      "[1, 13000] loss: 0.3211304859822849\n",
      "[1, 14000] loss: 0.3121015470894672\n",
      "[1, 15000] loss: 0.31297392972680066\n",
      "[2, 1000] loss: 0.2639582327926155\n",
      "[2, 2000] loss: 0.26170967807772283\n",
      "[2, 3000] loss: 0.2950111890168289\n",
      "[2, 4000] loss: 0.2904840947453445\n",
      "[2, 5000] loss: 0.2650629503286191\n",
      "[2, 6000] loss: 0.2706037216464465\n",
      "[2, 7000] loss: 0.3048746287730598\n",
      "[2, 8000] loss: 0.2683704923162849\n",
      "[2, 9000] loss: 0.26522221759694514\n",
      "[2, 10000] loss: 0.26267896112863615\n",
      "[2, 11000] loss: 0.2604888141651529\n",
      "[2, 12000] loss: 0.2736029814538443\n",
      "[2, 13000] loss: 0.2656185702275925\n",
      "[2, 14000] loss: 0.27677459797050274\n",
      "[2, 15000] loss: 0.2817556988348392\n",
      "[3, 1000] loss: 0.23208494675035513\n",
      "[3, 2000] loss: 0.23078243030176726\n",
      "[3, 3000] loss: 0.23843789640951263\n",
      "[3, 4000] loss: 0.23963975784101552\n",
      "[3, 5000] loss: 0.24789919887806655\n",
      "[3, 6000] loss: 0.2539566639243644\n",
      "[3, 7000] loss: 0.21096087417006287\n",
      "[3, 8000] loss: 0.24354586523411956\n",
      "[3, 9000] loss: 0.24718173365320376\n",
      "[3, 10000] loss: 0.2403214229674554\n",
      "[3, 11000] loss: 0.24698651917751613\n",
      "[3, 12000] loss: 0.24379278480644836\n",
      "[3, 13000] loss: 0.23527356024871468\n",
      "[3, 14000] loss: 0.24624157575184802\n",
      "[3, 15000] loss: 0.2332696245586684\n",
      "[4, 1000] loss: 0.19570207859583513\n",
      "[4, 2000] loss: 0.19323620309394268\n",
      "[4, 3000] loss: 0.1993209372021691\n",
      "[4, 4000] loss: 0.23189219349185572\n",
      "[4, 5000] loss: 0.2358657444184948\n",
      "[4, 6000] loss: 0.2127536040976316\n",
      "[4, 7000] loss: 0.21279853751208416\n",
      "[4, 8000] loss: 0.21163834885605423\n",
      "[4, 9000] loss: 0.19655504021011067\n",
      "[4, 10000] loss: 0.22134162287445514\n",
      "[4, 11000] loss: 0.21501748559151854\n",
      "[4, 12000] loss: 0.2255144743245362\n",
      "[4, 13000] loss: 0.22542255508153192\n",
      "[4, 14000] loss: 0.2370065307007132\n",
      "[4, 15000] loss: 0.2181771946337639\n",
      "[5, 1000] loss: 0.17244398876582143\n",
      "[5, 2000] loss: 0.18484260689146464\n",
      "[5, 3000] loss: 0.17710365928083452\n",
      "[5, 4000] loss: 0.19322989030061197\n",
      "[5, 5000] loss: 0.1859643455109409\n",
      "[5, 6000] loss: 0.1895063040159991\n",
      "[5, 7000] loss: 0.19956956927426522\n",
      "[5, 8000] loss: 0.193645066251417\n",
      "[5, 9000] loss: 0.19998621376252812\n",
      "[5, 10000] loss: 0.20247525730917512\n",
      "[5, 11000] loss: 0.21035763916791309\n",
      "[5, 12000] loss: 0.200189671606061\n",
      "[5, 13000] loss: 0.19188478247293336\n",
      "[5, 14000] loss: 0.20425367479128465\n",
      "[5, 15000] loss: 0.20458748168162832\n"
     ]
    }
   ],
   "source": [
    "# run training\n",
    "train_loader = torch.utils.data.DataLoader(fmnist_train, batch_size=4, shuffle=True)\n",
    "classifier = Classifier()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "train(classifier, train_loader, criterion, optimizer, epochs=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  3.9903, -10.9505,  -8.1856,  -6.4368, -12.2612, -21.7233,   1.7030,\n",
       "         -19.0155, -15.7663, -26.5770],\n",
       "        [-24.7924, -31.5912, -24.9703, -28.6228, -37.8264,  -6.2844, -21.3845,\n",
       "         -10.9280, -26.2253,   7.3938],\n",
       "        [ -6.0053, -17.1533,  -3.3409,  -8.0582,  -0.8078, -39.6625,   5.7848,\n",
       "         -36.5317, -24.0784, -41.7130],\n",
       "        [-29.1442, -24.4579, -38.2499, -42.0445, -52.1394, -16.4124, -38.1905,\n",
       "          -9.5167, -22.1381,  13.1875]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass on 1 example\n",
    "for img, lab in train_loader:\n",
    "    output = model(img)\n",
    "    break\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# copy model and cut out the last layer\n",
    "encoder = Classifier()\n",
    "encoder.load_state_dict(classifier.state_dict())\n",
    "encoder.fc2 = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# forward pass on 1 example\n",
    "for img, lab in train_loader:\n",
    "    output = encoder(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  5.4285,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0290,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.2115,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  9.5629,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  6.2770,  2.7229,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  6.8298,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000, 10.5283,  0.0000,  9.9873,  0.0000,  0.0000,  0.0000,  5.9944,\n",
       "         0.0000,  0.0000, 11.0462,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.1624,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  1.9188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.8459,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.5458,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  3.3182,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000, 11.3757,  5.3341,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_train[13][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def compare(dataset, i1, i2):\n",
    "    # get 2 random images and their labels\n",
    "    img1, lab1 = dataset[i1]\n",
    "    img2, lab2 = dataset[i2]\n",
    "\n",
    "    # forward pass on 2 images\n",
    "    output1 = encoder(img1.unsqueeze(0))\n",
    "    output2 = encoder(img2.unsqueeze(0))\n",
    "\n",
    "    # calculate cosine similarity\n",
    "    cos = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "    return lab1 == lab2, cos(output1, output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, tensor([0.8365], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(fmnist_train, 13, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, tensor([1.], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(fmnist_train, 13, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, tensor([0.4311], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(fmnist_train, 13, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, tensor([0.1742], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(fmnist_train, 13, 144)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
